#Navegaçao
    def get_slug(self, url_completa):

        url_limpa = url_completa.rstrip('/') 
        slug = url_limpa.split('/')[-1]
        return slug
    
        

    def open_url_empresa(self, slug_path):


    
        if slug_path.startswith('http'):
            empresa_URL = slug_path
        else:
            empresa_URL = self.base_url.rstrip('/') + slug_path

        try:
            self.driver.get(empresa_URL)
            return empresa_URL
        except Exception as e:
            print(f"ERRO DE NAVEGAÇÃO: Falha ao acessar {empresa_URL}. {e}")
            return None
       
        

#Extrai dados
    def extrai_data(self, empresa_data,  per_f = "últimos 6 meses é", per_label = "6 meses"):

        scraper.close()

        nome_empresa = empresa_data['Nome']
  
        data = empresa_data.copy()
        data ['Período'] = per_label     

        print(f"Extraindo dados da empresa: {nome_empresa}")

        teste_xpath_espera = f"//span[contains(text(), '{per_f}')]/b"
        try:
            self.wait.until(EC.presence_of_element_located((By.XPATH, teste_xpath_espera)))
        except:
            print(f"ERRO perfil {nome_empresa} não carrega")
            data['Nota Geral'] = "Pagina nao carregada"
            self.results.append(data)
            return
        html_conteudo = self.driver.page_source
        soup = BeautifulSoup(html_conteudo, 'html.parser')

        data_bs4 = {
        'Nota Geral': (lambda tag: tag.name == 'b' and tag.parent and per_f in tag.parent.text),
        'Reclamações respondidas (%)': (lambda tag: tag.name == 'strong' and 'Respondeu' in tag.parent.text),
        'Voltariam a fazer negócio (%)': (lambda tag: tag.name == 'strong' and 'Dos que avaliaram' in tag.parent.text),
        'Índice de solução (%)': (lambda tag: tag.name == 'strong' and 'A empresa resolveu' in tag.parent.text),
        'Nota do consumidor': (lambda tag: tag.name == 'strong' and 'Nota média' in tag.parent.text and tag.parent.find_all('strong')) 
        }

        for label, bs4_selector in data_bs4.items():
            try:
                elemento = soup.find(bs4_selector)
                if elemento:
                    valor_bruto = elemento.text.strip()
                    if label == 'Nota Geral' :
                        valor = valor_bruto.splitlines()[0]
                    else:
                        valor = valor_bruto.split(" ")[0]
                    data[label]= valor
                else:
                    data[label]= "N/A"
                    
            except Exception as e:
                data[label]= "N/A"

        self.results.append(data)
        print(f"Dados de {nome_empresa} extraidos com sucesso")

#troca de periodo
    def troca_periodo(self, periodo_nome):
        if periodo_nome =="6 meses":
            return True
    
        print("Alternando periodos")

        tab_xpath = f"//button[contains(text(), '{periodo_nome}')]"

        try:
            tab_elemento = self.wait.until(EC.element_to_be_clickable((By.XPATH, tab_xpath)))
            self.driver.execute_script("arguments[0].click();", tab_elemento)
            print(f"Alternado para {periodo_nome}")
            return True
        except:
            print(f"Aba {periodo_nome} não encontrada")
            return False







            scraper = None
        exporter = ExportaData()
        try:
            scraper = ReclameAquiScraper()
            scraper.open_site()
            companies = scraper.get_empresa_lists()

            periodos = {
                "6 meses": "últimos 6 meses é", 
                "12 meses": "últimos 12 meses é",
                "2024": "2024 é",
                "2023": "2023 é",
                "Geral": "geral é"
            }

            for empresa_data in companies:
                for per_label, per_f in periodos.items():
                    if scraper.troca_periodo(per_label):
                        scraper.extrai_data(
                            empresa_data.copy(),
                            per_f=per_f,
                            per_label=per_label
                        )
            print("exportando para excel...")
            exporter.export_to_excel(scraper.results)


        except Exception as e:
            print(f"erro final {e}")
        finally:
            if scraper:
                scraper.close()